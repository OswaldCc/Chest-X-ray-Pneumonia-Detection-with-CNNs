{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMAGE CLASSIFICATION WITH DEEP LEARNING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Data Analytic Question & Problem Statement\n",
    "\n",
    "Tibabu hospital has observed an increase in Pneumonia infections. The hospital is understaffed, and cannot keep up with the high volume of patients. They approached Avengers LTD for a solution to help with the diagnosis. So to improve diagnosis of pneumonia using x-rays, tasked Avengers to come up with a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Problem Context\n",
    "Pneumonia is a serious lung condition that can be brought on by bacteria, viruses, or fungus. When someone has pneumonia, the alveoli (air sacs) are filled with fluid and pus, which makes breathing unpleasant, difficult, and restricts oxygen intake (reduced oxygen saturation).\n",
    "According to the WHO, pneumonia caused 14% of all deaths among children under the age of five in 2019. Both young people and the elderly are susceptible.\n",
    "A patient's clinical examination is used to diagnose pneumonia.\n",
    "\n",
    "Machine learning in healthcare reduces the possibility of doing an improper examination. Therefore, Tibabu Hospital would like to put into practice a methodology that would enable them to detect pneumonia more precisely utilizing chest X-ray pictures. As a machine learning algorithm would sift through a vast amount of data to learn the most likely diagnosis, this would give patients more confidence in the diagnosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Metric of Success\n",
    "The algorithm will be considered a success if the predictions made by it have a:\n",
    "\n",
    "Recall above 95%\n",
    "Accuracy of 90% - 96%\n",
    "The task is to build a model that can classify whether a given patient has pneumonia given their chest x-ray image.\n",
    "\n",
    "Since this is an Image Classification problem, Deep Learning is the ideal solving method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Data Understanding\n",
    "\n",
    "The medical dataset contains a set of x-ray images of pediatric patients. \n",
    "The images are of patients who have pneumonia and those who don't.\n",
    "\n",
    "Chest X-ray images were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou, China. All chest X-ray imaging were performed as part of patients’ routine clinical care.\n",
    "\n",
    "For the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n",
    "\n",
    "The final data is in two classes, Normal and Pneumonia. This dataset was downloaded from [Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia). The data was further divided into three folders of  train, test and validation, each with two classes; normal and pneumonia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Project Plan\n",
    "\n",
    "1. Load and Pre-process the data.\n",
    "2. Define the trained generator, validation generator, and test generator.\n",
    "3. Build a baseline model.\n",
    "4. Tune iterative models.\n",
    "5. Challenge the solution with Transfer Learning . \n",
    "6. Evaluate the model with the test images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Loading the Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to Google drive and Importing all the necessary Libraries | Modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all relevant libraries\n",
    "\n",
    "import numpy as np \n",
    "import os, shutil\n",
    "\n",
    "import time\n",
    "\n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.applications import VGG19, MobileNet, resnet50\n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator, \\\n",
    "                                                 array_to_img\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, MaxPooling2D,\\\n",
    "                                    Conv2D, Flatten, Activation, Dropout, Rescaling\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau , ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import recall_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Loading the Data\n",
    "\n",
    "The data was loaded using `os.listdir` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the contents of the chest_xray folder \n",
    "\n",
    "print(os.listdir('/content/drive/Shareddrives/Avengers/chest_xray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stored the data for both normal and pneumonia x-ray images into variables for easy access. \n",
    "\n",
    "This was done for both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the directory paths as variables\n",
    "\n",
    "train_norm_dir = '/content/drive/Shareddrives/Avengers/chest_xray/train/NORMAL'\n",
    "train_pneu_dir = '/content/drive/Shareddrives/Avengers/chest_xray/train/PNEUMONIA'\n",
    "\n",
    "test_norm_dir = '/content/drive/Shareddrives/Avengers/chest_xray/test/NORMAL'\n",
    "test_pneu_dir = '/content/drive/Shareddrives/Avengers/chest_xray/test/PNEUMONIA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of images in the directories was then created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store train normal xrays and pneumonia xrays as variables.\n",
    "\n",
    "train_norm_images = os.listdir(train_norm_dir)\n",
    "train_pneu_images = os.listdir(train_pneu_dir)\n",
    "\n",
    "# store train normal xrays and pneumonia x-rays as variables.\n",
    "\n",
    "test_norm_images = os.listdir(test_norm_dir)\n",
    "test_pneu_images = os.listdir(test_pneu_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Previewing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image from the train NORMAL folder.\n",
    "\n",
    "\n",
    "img_name_normal = '/' + train_norm_images[3]\n",
    "normal_lung = load_img(train_norm_dir + img_name_normal)\n",
    "\n",
    "# normal chest x-ray\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(normal_lung)\n",
    "plt.title('Normal chest x-ray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image from the train PNEUMONIA folder.\n",
    "\n",
    "img_name_pneumonia = '/' + train_pneu_images[3]\n",
    "pneumonia_lung = load_img(train_pneu_dir + img_name_pneumonia)\n",
    "\n",
    "# pneumonia chest x-ray\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(pneumonia_lung)\n",
    "plt.title('Pneumonia chest x-ray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted the class distributions to further understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the training data\n",
    "\n",
    "total_train = train_norm_images + train_pneu_images\n",
    "print('Total train data size is:', np.shape(total_train)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating countplot for total_counts_train\n",
    "\n",
    "total_counts_train = []\n",
    "pneu_len_train = 0\n",
    "norm_len_train = 0\n",
    "\n",
    "for i in total_train:\n",
    "    if i[0] == 'p':\n",
    "        total_counts_train.append('Pneumonia')\n",
    "        pneu_len_train += 1\n",
    "    elif i[0] == 'N' or i[0] == 'I':\n",
    "        total_counts_train.append('Normal')\n",
    "        norm_len_train += 1\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "title_font_dict = {'size': 22}\n",
    "labels_font_dict = {'size': 16}\n",
    "\n",
    "sns.countplot(x=total_counts_train)\n",
    "\n",
    "plt.ylabel('Count', labels_font_dict)\n",
    "plt.xticks(fontsize = 14 )\n",
    "plt.yticks(fontsize = 14 )\n",
    "\n",
    "plt.title('Distribution of Classes in Train Data', title_font_dict)\n",
    "plt.show()\n",
    "\n",
    "print('\\nTotal count of normal chest x-rays: {}'.format(norm_len_train))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print('Total count of pneumonia chest x-rays: {}'.format(pneu_len_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noted that the training data is imbalanced at a ratio of ~ 1:3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all the test data \n",
    "\n",
    "total_test = test_norm_images + test_pneu_images\n",
    "print('Total test data size is:', np.shape(total_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating countplot for total_counts_test\n",
    "\n",
    "total_counts_test= []\n",
    "pneu_len_test = 0\n",
    "norm_len_test = 0\n",
    "\n",
    "for i in total_test:\n",
    "    if i[0] == 'p':\n",
    "        total_counts_test.append('Pneumonia')\n",
    "        pneu_len_test += 1\n",
    "    elif i[0] == 'N' or i[0] == 'I':\n",
    "        total_counts_test.append('Normal')\n",
    "        norm_len_test += 1\n",
    "        \n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "sns.countplot(x=total_counts_test)\n",
    "\n",
    "plt.ylabel('Count', labels_font_dict)\n",
    "plt.xticks(fontsize = 14 )\n",
    "plt.yticks(fontsize = 14 )\n",
    "\n",
    "plt.title('Distribution of Classes in Test Data', title_font_dict)\n",
    "plt.show()\n",
    "\n",
    "print('\\nTotal count of normal chest x-rays: {}'.format(norm_len_test))\n",
    "print('----------------------------------------------------------------------------------')\n",
    "print('Total count of pneumonia chest x-rays: {}'.format(pneu_len_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data has an imbalance of ratio ~ 1:2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Reshaping and Normalizing the Data\n",
    "\n",
    "Reshaping is important for pixel uniformity in the images, while normalization makes computations efficient by reducing values between 0 to 1. This will be achieved using the `ImageDataGenerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store test and train directory paths as variables\n",
    "\n",
    "train_dir = '/content/drive/Shareddrives/Avengers/chest_xray/train'\n",
    "test_dir = '/content/drive/Shareddrives/Avengers/chest_xray/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting some global variables\n",
    "\n",
    "image_height = 156\n",
    "image_width = 156\n",
    "\n",
    "input_shape = (image_height, image_width, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescaling all images\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # this is the target directory for train data\n",
    "        train_dir,\n",
    "        # all images will be resized to 156*156 pixels\n",
    "        target_size = (image_height, image_width),\n",
    "        batch_size = len(total_train),\n",
    "        # using binary labels because target is binary\n",
    "        class_mode = 'binary')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        # this is the target directory for test data\n",
    "        test_dir,\n",
    "        # all images will be resized to 156*156 pixels\n",
    "        target_size = (image_height, image_width),\n",
    "        batch_size = len(total_test),\n",
    "        # using binary labelsbecause target is binary\n",
    "        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating preprocessed datasets\n",
    "\n",
    "train_images, train_labels = next(train_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing an xray from the new dataset.\n",
    "array_to_img(train_images[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the labels\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previewing shape of preprocessed data\n",
    "\n",
    "print('The new train shape is:', train_images.shape)\n",
    "print('The new test shape is:', test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifying normal and pneumonia label notations\n",
    "\n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the number of classes \n",
    "\n",
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementing the Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identified solution to the problem statement is modelling the given data. Given that this is an image classification problem, Convolutional Neural Networks (CNN) would be ideal as they have better performance compared to other neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_visualizations(fitted_model):\n",
    "    \"\"\"\n",
    "    Takes in a fitted model and returns a plot of all scores used in said model.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(28, 8))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, score in enumerate(['recall', 'accuracy', 'loss']):\n",
    "        if max(fitted_model.history[score]) >= 0.6:\n",
    "            # Plot for train scores\n",
    "            ax[i].plot(fitted_model.history[score])\n",
    "            # plot for val/test scores\n",
    "            ax[i].plot(fitted_model.history['val_' + score])\n",
    "\n",
    "            ax[i].set_title('Model {}'.format(score).title(), title_font_dict)\n",
    "            ax[i].set_xlabel('Epochs', labels_font_dict)\n",
    "            ax[i].set_ylabel(score, labels_font_dict)\n",
    "            ax[i].set_ylim([0.6, max(fitted_model.history[score]) + 0.05])\n",
    "\n",
    "            ax[i].legend(['train', 'val']) \n",
    "\n",
    "        elif max(fitted_model.history[score]) < 0.6:\n",
    "            # Plot for train scores\n",
    "            ax[i].plot(fitted_model.history[score])\n",
    "            # plot for val/test scores\n",
    "            ax[i].plot(fitted_model.history['val_' + score])\n",
    "\n",
    "            ax[i].set_title('Model {}'.format(score).title(), title_font_dict)\n",
    "            ax[i].set_xlabel('Epochs', labels_font_dict)\n",
    "            ax[i].set_ylabel(score, labels_font_dict)\n",
    "            ax[i].set_ylim(0)\n",
    "\n",
    "            ax[i].legend(['train', 'val']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 7.1 Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a baseline model\n",
    "\n",
    "baseline_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding layers to the baseline model\n",
    "\n",
    "baseline_model.add(Input(shape = input_shape))\n",
    "\n",
    "baseline_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "baseline_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "baseline_model.add(Flatten())\n",
    "baseline_model.add(Dense(64, activation='relu'))\n",
    "\n",
    "baseline_model.add(Dense(num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the baseline model\n",
    "\n",
    "baseline_model.compile(optimizer ='adam',\n",
    "                       loss = \"binary_crossentropy\",\n",
    "                       metrics = [tf.keras.metrics.Recall(name = 'recall'), \n",
    "                                  'accuracy']\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit baseline model\n",
    "\n",
    "base_start = time.time()\n",
    "\n",
    "base_result = baseline_model.fit(train_images, \n",
    "                                 train_labels, \n",
    "                                 epochs = 10, \n",
    "                                 batch_size = 32, \n",
    "                                 validation_split = 0.2 \n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_stop = time.time()\n",
    "print(f'Time taken to fit the baseline model was {round(base_stop - base_start, 3)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_visualizations(base_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model had a great performance with:\n",
    "\n",
    "      Runtime = 42.852 seconds\n",
    "\n",
    "      Recall: Train - 100%, \n",
    "              Validation - ~98%\n",
    "\n",
    "      loss: Train - 0.0009648, \n",
    "            Validation - 0.0839\n",
    "      \n",
    "      Accuracy: Train - 100%, \n",
    "                Validation - ~97%\n",
    "\n",
    "The data used has an imbalance, dealing with it may improve the modelling results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Tuned Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create preprocessing layers for data augmentation \n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment the train dataset\n",
    "\n",
    "augmented_image = data_augmentation(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tuned baseline model\n",
    "\n",
    "aug_start = time.time()\n",
    "\n",
    "base_tuned_result = baseline_model.fit(augmented_image, \n",
    "                                 train_labels , \n",
    "                                 epochs = 10, \n",
    "                                 batch_size = 32, \n",
    "                                 validation_split = 0.2 \n",
    "                                 )\n",
    "aug_stop = time.time()\n",
    "print(f'Time taken to fit the tuned baseline model was {round(aug_stop - aug_start, 3)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_visualizations(base_tuned_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmenting the data did not improve the model performance. \n",
    "\n",
    "        Runtime = 29.157 seconds\n",
    "        \n",
    "        Recall dropped from ~98% to ~94%. \n",
    "\n",
    "        Validation Loss has had a considerable increase from 0.0839 to 0.3845\n",
    "\n",
    "        Accuracy dropped from ~97% to ~89%.\n",
    "\n",
    "\n",
    "Therefore, Model optimization proceeds with the imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Third Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the third model\n",
    "\n",
    "third_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding layers to the third model\n",
    "\n",
    "third_model.add(Input(shape=input_shape))\n",
    "\n",
    "third_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "third_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "third_model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "third_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "third_model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "third_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "third_model.add(Flatten())\n",
    "third_model.add(Dense(64, activation='relu'))\n",
    "third_model.add(Dropout(0.5))\n",
    "\n",
    "third_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the third model\n",
    "\n",
    "third_model.compile(optimizer ='adam',\n",
    "                    loss = \"binary_crossentropy\",\n",
    "                    metrics = [tf.keras.metrics.Recall(name = 'recall'), \n",
    "                               'accuracy']\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the third model\n",
    "\n",
    "third_start = time.time()\n",
    "\n",
    "third_result = third_model.fit(train_images, \n",
    "                               train_labels, \n",
    "                               epochs = 10, \n",
    "                               batch_size = 32, \n",
    "                               validation_split = 0.2 \n",
    "                               )\n",
    "\n",
    "third_stop = time.time()\n",
    "print(f'Time taken to fit the baseline model was {round(third_stop - third_start, 3)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_visualizations(third_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a better runtime and scores than the baseline model:\n",
    "\n",
    "        Runtime = 36.481 seconds\n",
    "\n",
    "        Recall: Train - ~98%, \n",
    "                Validation - ~98%\n",
    "\n",
    "        loss: Train - 0.0689, \n",
    "              Validation - 0.0895\n",
    "\n",
    "        Accuracy: Train - ~97%, \n",
    "                  Validation - ~97%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis of the three models, the third model had the shortest runtime and the scores suggest that there is no overfitting or underfitting happening.\n",
    "\n",
    "Based on that, this is the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions using the third model\n",
    "\n",
    "final_model = third_model\n",
    "\n",
    "final_scores = final_model.evaluate(test_images, test_labels)\n",
    "\n",
    "# checking the scores \n",
    "\n",
    "print('\\nThe final recall score is:', final_scores[1])\n",
    "print('-'*40)\n",
    "print('The final accuracy score is:', final_scores[2])\n",
    "print('-'*40)\n",
    "print('The final loss is:', final_scores[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenging the Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transfer learning model was selected to challenge the final model. \n",
    "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. \n",
    "\n",
    "One can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
    "\n",
    "VGG19 was created by Oxford, and has 19 layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insantiating the model\n",
    "\n",
    "vgg_model = tf.keras.applications.VGG19(weights='imagenet', \n",
    "                                        include_top = False,\n",
    "                                        input_shape = input_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freezing the layers\n",
    "\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding layers\n",
    "\n",
    "x = vgg_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = tf.keras.layers.Dense(128,activation='relu')(x)\n",
    "# output layer\n",
    "predictions = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=vgg_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid overfitting\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "\n",
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.Recall(name = 'recall'), \n",
    "                       'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "\n",
    "vgg_start = time.time()\n",
    "\n",
    "vgg_results = model.fit(train_images ,\n",
    "                        train_labels,\n",
    "                        epochs=10, \n",
    "                        validation_split = 0.2, \n",
    "                        steps_per_epoch=100,\n",
    "                        callbacks=[early_stopping,lr],\n",
    "                        batch_size=32)\n",
    "\n",
    "vgg_stop = time.time()\n",
    "print(f'Time taken to fit the VGG model was {round(vgg_stop - vgg_start, 3)} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_visualizations(vgg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivity of detecting pneumonia by emergency medicine specialists and radiologists [according to this paper](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6377225/#:~:text=Sensitivity%20of%20plain%20chest%20radiography,radiation%20(9%2C%2010)) is 83%,  therefore: \n",
    "- Given that the model has  98% sensitivity, interpretation of chest x-rays using this Convolutional Algorithm  might help in improving the diagnostic accuracy of pneumonia.\n",
    "- The algorithm is fast (36s) hence will increase efficiency considering the current understaffing in the Hospital."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be deployed for use in the hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
